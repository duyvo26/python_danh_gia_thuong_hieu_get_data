import requests
import asyncio
from crawl4ai import *  # noqa: F403
import aiofiles
import time
from urllib.parse import urlparse
from app.config import settings
import os
from bs4 import BeautifulSoup


def check_captcha(html):
    if "/recaptcha/".lower() in html.lower():
        return False
    if 'id="captcha-form"'.lower() in html.lower():
        return False
    if "https://www.google.com/recaptcha/api.js".lower() in html.lower():
        return False
    return True




import os

PROXY_FILE_PATH = "proxy_index.txt"


def read_proxy_index():
    try:
        with open(PROXY_FILE_PATH, "r") as f:
            return int(f.read().strip())
    except (FileNotFoundError, ValueError):
        return 0  # M·∫∑c ƒë·ªãnh l√† 0 n·∫øu ch∆∞a c√≥ file


def write_proxy_index(index):
    with open(PROXY_FILE_PATH, "w") as f:
        f.write(str(index))


def get_proxy_config_from_env():
    index = read_proxy_index()
    next_index = (index + 1) % 2  # Gi·∫£ s·ª≠ c√≥ 2 proxy, b·∫°n c√≥ th·ªÉ m·ªü r·ªông n·∫øu c√≥ nhi·ªÅu h∆°n

    # L∆∞u l·∫°i index cho l·∫ßn g·ªçi ti·∫øp theo
    write_proxy_index(next_index)

    # ƒê·ªçc th√¥ng tin t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
    prefix = "" if index == 0 else f"_{index}"
    proxy = {
        "username": os.getenv(f"PROXY_USERNAME{prefix}"),
        "password": os.getenv(f"PROXY_PASSWORD{prefix}"),
        "host": os.getenv(f"PROXY_HOST{prefix}"),
        "port": os.getenv(f"PROXY_PORT{prefix}"),
    }

    return ProxyConfig(  # noqa: F405
        server=f"http://{proxy['host']}:{proxy['port']}",
        username=proxy["username"],
        password=proxy["password"],
    )
    
    
async def crawl4ai_run_proxy(url, proxy=False):
    print(f"ƒêang d√πng proxy")  # noqa: F541

    proxy_config = get_proxy_config_from_env()

    browser_config = BrowserConfig(  # noqa: F405
        proxy_config=proxy_config,
        headless=True,
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:  # noqa: F405
        result = await crawler.arun(
            url=url,
            config=CrawlerRunConfig(  # noqa: F405
                cache_mode=CacheMode.BYPASS  # noqa: F405
            ),
        )

        markdown_content = result.markdown
        html_content = result.html  # üëà Ph·∫£i c√≥ html m·ªõi parse meta ƒë∆∞·ª£c!

        meta = extract_meta(html_content)

        # T·∫°o t√™n file ƒë·ªông
        timestamp = int(time.time())
        domain = urlparse(url).netloc.replace(".", "_")
        filename = f"{domain}_{timestamp}.md"
        path_file = os.path.join(settings.DIR_ROOT, "utils", "web", filename)

        async with aiofiles.open(path_file, mode="w", encoding="utf-8") as f:
            await f.write(markdown_content)

        return {
            "markdown": markdown_content,
            "meta": meta,
        }


async def crawl4ai_run(url):
    async with AsyncWebCrawler() as crawler:  # noqa: F405
        result = await crawler.arun(
            url=url,
            config=CrawlerRunConfig(  # noqa: F405
                cache_mode=CacheMode.BYPASS  # noqa: F405
            ),
        )

        markdown_content = result.markdown
        html_content = result.html

        meta = extract_meta(html_content)

        # T·∫°o t√™n file ƒë·ªông
        timestamp = int(time.time())
        domain = urlparse(url).netloc.replace(".", "_")
        filename = f"{domain}_{timestamp}.md"
        path_file = os.path.join(settings.DIR_ROOT, "utils", "web", filename)

        async with aiofiles.open(path_file, mode="w", encoding="utf-8") as f:
            await f.write(markdown_content)

        return {
            "markdown": markdown_content,
            "meta": meta,
        }


def extract_meta(html):
    soup = BeautifulSoup(html, "html.parser")
    title = soup.title.string.strip() if soup.title else ""
    description = ""
    keywords = ""

    desc_tag = soup.find("meta", attrs={"name": "description"})
    if desc_tag:
        description = desc_tag.get("content", "")

    keywords_tag = soup.find("meta", attrs={"name": "keywords"})
    if keywords_tag:
        keywords = keywords_tag.get("content", "")

    return {
        "title": title,
        "description": description,
        "keywords": keywords,
    }


def response_custom(url, proxy=False):
    try:
        if proxy:
            crawl_result = asyncio.run(crawl4ai_run_proxy(url))
        else:
            crawl_result = asyncio.run(crawl4ai_run(url))
        return crawl_result
    except Exception as e:
        print(f"L·ªói khi g·ª≠i y√™u c·∫ßu: {e}")
        return None
